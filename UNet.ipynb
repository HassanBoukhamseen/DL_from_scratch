{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"UNet.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","mount_file_id":"14MGPabERomkZZHZbrPSYusN0_2au4Rk9","authorship_tag":"ABX9TyMlgKZoLu0xabnh6ItI8aYz"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"0I5v0AVAArgj"},"source":["import torch\n","import torch.nn as nn\n","from torch import optim\n","import torchvision\n","from torchvision import datasets\n","from torchvision.transforms import ToTensor, Compose, Normalize, RandomRotation, Resize\n","from torch.utils.data import DataLoader\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import math"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"doY-r-Udu0QH","executionInfo":{"status":"ok","timestamp":1627724521259,"user_tz":-240,"elapsed":629,"user":{"displayName":"Hassan Boukhamseen","photoUrl":"","userId":"13575020213521366718"}},"outputId":"45162061-8d68-4e31-a4d6-0eceed8a81f0"},"source":["from google.colab import drive\n","drive.mount(\"/content/drive\", force_remount=True)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"mNl9ac3hwcVt"},"source":["train_path = '/content/drive/MyDrive/chest_xray/train'\n","val_path = '/content/drive/MyDrive/chest_xray/val'\n","test_path =  '/content/drive/MyDrive/chest_xray/test'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"94LNgkG8w7vq"},"source":["DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","LR = 1e-3\n","IMG_SIZE = 224\n","BATCH_SIZE = 4\n","EPOCHS = 20"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lgk_aBCCwrmp"},"source":["train_transform = Compose([\n","                             \n","                             Resize(size=(IMG_SIZE,IMG_SIZE)), # Resizing the image to be 224 by 224\n","                             RandomRotation(degrees=(-20,+20)), #Randomly Rotate Images by +/- 20 degrees, Image argumentation for each epoch\n","                             ToTensor(), #converting the dimension from (height,weight,channel) to (channel,height,weight) convention of PyTorch\n","                             Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]) # Normalize by 3 means 3 StD's of the image net, 3 channels\n","\n","])\n","\n","validate_transform = Compose([\n","                             \n","                             Resize(size=(IMG_SIZE,IMG_SIZE)), # Resizing the image to be 224 by 224\n","                             ToTensor(), #converting the dimension from (height,weight,channel) to (channel,height,weight) convention of PyTorch\n","                             Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]) # Normalize by 3 means 3 StD's of the image net, 3 channels\n","\n","])\n","\n","test_transform = Compose([\n","                             \n","                             Resize(size=(IMG_SIZE,IMG_SIZE)), # Resizing the image to be 224 by 224\n","                             ToTensor(), #converting the dimension from (height,weight,channel) to (channel,height,weight) convention of PyTorch\n","                             Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]) # Normalize by 3 means 3 StD's of the image net, 3 channels\n","\n","])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"z6eWCJGnxbH6","executionInfo":{"status":"ok","timestamp":1627724521260,"user_tz":-240,"elapsed":11,"user":{"displayName":"Hassan Boukhamseen","photoUrl":"","userId":"13575020213521366718"}},"outputId":"19748cb5-86fb-474c-8fb4-247a27821063"},"source":["train_dataset = datasets.ImageFolder(train_path,transform=train_transform)\n","print(\"Trainset Size:  {}\".format(len(train_dataset)))\n","\n","val_dataset = datasets.ImageFolder(val_path,transform=validate_transform)\n","print(\"validateset Size:  {}\".format(len(val_dataset)))\n","\n","#test_dataset = datasets.ImageFolder(test_path,transform=test_transform)\n","#print(\"validateset Size:  {}\".format(len(test_dataset)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Trainset Size:  5193\n","validateset Size:  16\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vf6zDFVKzEHg","executionInfo":{"status":"ok","timestamp":1627724521261,"user_tz":-240,"elapsed":10,"user":{"displayName":"Hassan Boukhamseen","photoUrl":"","userId":"13575020213521366718"}},"outputId":"4a0d9ce8-8ae6-4403-bc7f-5f12fc456ab1"},"source":["trainloader = DataLoader(train_dataset,batch_size=BATCH_SIZE,shuffle=True)\n","print(\"No. of batches in trainloader:{}\".format(len(trainloader))) #Trainset Size:  5216 / batch_size: 16 = 326(No. of batches in trainloader) \n","print(\"No. of Total examples:{}\".format(len(trainloader.dataset)))\n","\n","validationloader = DataLoader(val_dataset,batch_size=BATCH_SIZE,shuffle=True)\n","print(\"No. of batches in validationloader:{}\".format(len(validationloader))) #validationset Size:  16 / batch_size: 16 = 1(No. of batches in validationloader) \n","print(\"No. of Total examples:{}\".format(len(validationloader.dataset)))\n","\n","#testloader = DataLoader(test_dataset,batch_size=BATCH_SIZE,shuffle=True)\n","#print(\"No. of batches in testloader:{}\".format(len(testloader))) #testset Size:  624 / batch_size: 16 = 39(No. of batches in testloader) \n","#print(\"No. of Total examples:{}\".format(len(testloader.dataset)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["No. of batches in trainloader:1299\n","No. of Total examples:5193\n","No. of batches in validationloader:4\n","No. of Total examples:16\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"mx3VebGABI2p"},"source":["class DoubleConv(nn.Module):\n","    def __init__(self, in_channels, out_channels, kernel_size=3):\n","        super(DoubleConv, self).__init__()\n","        self.conv = nn.Sequential(\n","            nn.Conv2d(in_channels, out_channels, kernel_size), \n","            nn.ReLU(inplace=True), \n","            nn.BatchNorm2d(out_channels),\n","            nn.Conv2d(out_channels, out_channels, kernel_size), \n","            nn.ReLU(inplace=True), \n","            nn.BatchNorm2d(out_channels),\n","        )\n","    def forward(self, x):\n","        return self.conv(x)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jmbEA2OrU220"},"source":["def crop_and_concat(tens1, tens2):\n","    tens1_size = tens1.size()[2]\n","    tens2_size = tens2.size()[2]\n","    diff = (tens1_size - tens2_size) // 2\n","    tens1 = tens1[:, :, diff:tens1_size-diff, diff:tens1_size-diff]\n","    return torch.cat([tens1, tens2], dim=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NtkXqVWWBd1n"},"source":["class UNet(nn.Module):\n","    def __init__(self, in_channels=3, out_channels=1):\n","        super(UNet, self).__init__()\n","        self.max_pool = nn.MaxPool2d(2, 2)\n","        # encoder\n","        self.down_1 = nn.Sequential(\n","            nn.Conv2d(3, 64, kernel_size=3, padding=174), \n","            nn.ReLU(inplace=True), \n","            nn.BatchNorm2d(64),\n","            nn.Conv2d(64, 64, kernel_size=3), \n","            nn.ReLU(inplace=True), \n","            nn.BatchNorm2d(64),\n","        )\n","        self.down_2 = DoubleConv(64, 128)\n","        self.down_3 = DoubleConv(128, 256)\n","        self.down_4 = DoubleConv(256, 512)\n","        # bottle_neck\n","        self.conv_1 = nn.Conv2d(512, 1024, 3)\n","        self.conv_2 = nn.Conv2d(1024, 1024, 3)\n","        # decoder\n","        self.up_conv_1 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n","        self.up_1 = DoubleConv(1024, 512)\n","        self.up_conv_2 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n","        self.up_2 = DoubleConv(512, 256)\n","        self.up_conv_3 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n","        self.up_3 = DoubleConv(256, 128)\n","        self.up_conv_4 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n","        self.up_4 = DoubleConv(128, 64)\n","        \n","        #output\n","        self.one_by_one_conv = nn.Conv2d(64,1, 1)\n","\n","    def forward(self, x):\n","        #decoder\n","        x1 = self.down_1(x)\n","        x2 = self.max_pool(x1)\n","\n","        x3 = self.down_2(x2)\n","        x4 = self.max_pool(x3)\n","\n","        x5 = self.down_3(x4)\n","        x6 = self.max_pool(x5)\n","\n","        x7 = self.down_4(x6)\n","        x8 = self.max_pool(x7)\n","\n","        #bottle_neck\n","        x9 = self.conv_1(x8)\n","        x10 = self.conv_2(x9)\n","\n","        #encoder\n","        x11 = self.up_conv_1(x10)\n","        x12 = crop_and_concat(x7, x11)\n","        x13 = self.up_1(x12)\n","\n","        x14 = self.up_conv_2(x13)\n","        x15 = crop_and_concat(x5, x14)\n","        x16 = self.up_2(x15)\n","\n","        x17 = self.up_conv_3(x16)\n","        x18 = crop_and_concat(x3, x17)\n","        x19 = self.up_3(x18)\n","\n","        x20 = self.up_conv_4(x19)\n","        x21 = crop_and_concat(x1, x20)\n","        x22 = self.up_4(x21)\n","        \n","        #output\n","        x22 = self.one_by_one_conv(x22)\n","        return x22"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fZ6Epor2Byls"},"source":["model = UNet().to(DEVICE)\n","optimizer = optim.Adam(model.parameters(), lr=LR)\n","bce = nn.BCEWithLogitsLoss()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"szHpft7j3oJk"},"source":["def validate(loader, model, device=\"cuda\"):\n","    num_correct = 0\n","    num_pixels = 0\n","    dice_score = 0\n","    with torch.no_grad():\n","        for x, y in loader:\n","            x = x.to(device)\n","            y = y.to(device).unsqueeze(1)\n","            preds = torch.sigmoid(model(x))\n","            preds = (preds > 0.5).float()\n","            num_correct += (preds == y).sum()\n","            num_pixels += torch.numel(preds)\n","            dice_score += (2 * (preds * y).sum()) / (\n","                (preds + y).sum() + 1e-8\n","            )\n","    print(f\"Got {num_correct}/{num_pixels} with acc {num_correct/num_pixels*100:.2f}\")\n","    print(f\"Dice score: {dice_score/len(loader)}\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"R84sZDgU3ozN"},"source":["def train(model, data, epochs=EPOCHS, device=DEVICE):\n","    for e in range(epochs):\n","        running_loss = 0\n","        for idx, (images, labels) in enumerate(data):\n","            images, labels = images.to(device), labels.to(device)\n","            output = model(images)\n","            optimizer.zero_grad()\n","            labels = labels.unsqueeze(1)\n","            loss = bce(output, labels)\n","            loss.backward()\n","            optimizer.step()\n","        validate(model, validationloader)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"arAGiH0FCFrI","colab":{"base_uri":"https://localhost:8080/","height":375},"executionInfo":{"status":"error","timestamp":1627724524393,"user_tz":-240,"elapsed":2362,"user":{"displayName":"Hassan Boukhamseen","photoUrl":"","userId":"13575020213521366718"}},"outputId":"f4b4cc6d-5f1f-4290-8308-236b11b3de5a"},"source":["'''\n","x = torch.randn((1, 3, 224, 224)).to(DEVICE)\n","output = model(x)\n","print(output.shape)\n","'''\n","#train(model, trainloader, epochs=30)"],"execution_count":null,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-233-0063e0bf4745>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m '''\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-232-54e534e91bbc>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, data, epochs, device)\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    714\u001b[0m                                                   \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m                                                   \u001b[0mpos_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 716\u001b[0;31m                                                   reduction=self.reduction)\n\u001b[0m\u001b[1;32m    717\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbinary_cross_entropy_with_logits\u001b[0;34m(input, target, weight, size_average, reduce, reduction, pos_weight)\u001b[0m\n\u001b[1;32m   2956\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2957\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2958\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Target size ({}) must be the same as input size ({})\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2959\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2960\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_cross_entropy_with_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction_enum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Target size (torch.Size([4, 1])) must be the same as input size (torch.Size([4, 1, 388, 388]))"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I--pc0yLDE_1","executionInfo":{"status":"ok","timestamp":1627726336786,"user_tz":-240,"elapsed":1232,"user":{"displayName":"Hassan Boukhamseen","photoUrl":"","userId":"13575020213521366718"}},"outputId":"9e2ddc20-5425-4ac8-f19c-f9b4c229a7e9"},"source":["!kaggle competitions download -c carvana-image-masking-challenge"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Traceback (most recent call last):\n","  File \"/usr/local/bin/kaggle\", line 5, in <module>\n","    from kaggle.cli import main\n","  File \"/usr/local/lib/python2.7/dist-packages/kaggle/__init__.py\", line 23, in <module>\n","    api.authenticate()\n","  File \"/usr/local/lib/python2.7/dist-packages/kaggle/api/kaggle_api_extended.py\", line 146, in authenticate\n","    self.config_file, self.config_dir))\n","IOError: Could not find kaggle.json. Make sure it's located in /root/.kaggle. Or use the environment method.\n"],"name":"stdout"}]}]}